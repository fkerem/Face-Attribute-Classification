{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceAttributeClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lvIQgrRzoj66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem Statement:\n",
        "\n",
        "### In this project, we have to take aligned images from CelebA dataset and detect whether they have specific attributes.\n",
        "### The attributes we are interested in are: (male/female; smile/not; mustache-beard/not; bangs/not; eyeglasses/not; hat/not; blonde/not; pale skin; attractive/not; blurry/not).\n",
        "\n",
        "### This is basically a multi-label classification problem which aims to target multiple attributes.\n",
        "### Therefore, we will develop a model which have an output layer with 10 neurons."
      ]
    },
    {
      "metadata": {
        "id": "1N4rdqjnYV0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install a drive fuse wrapper\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# Create a directory \"drive\" and mount Google Drive from it\n",
        "!mkdir -p drive\n",
        "#!ls drive/\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhFrFMIuaKC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('My Google Drive/Datasets Files:') # To test whether the drive folder is mounted in Colab.\n",
        "!ls drive/Datasets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hMKKPb4aNDT",
        "colab_type": "code",
        "outputId": "962a2b4c-2490-4fc5-9cb6-3890ccbd2738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import needed libraries to run CNN\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.applications import VGG16 \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "from keras.preprocessing import image\n",
        "\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qzeTkt9Enhnm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset Reference: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
        "\n",
        "####S. Yang, P. Luo, C. C. Loy, and X. Tang, \"From Facial Parts Responses to Face Detection: A Deep Learning Approach\", in IEEE International Conference on Computer Vision (ICCV), 2015"
      ]
    },
    {
      "metadata": {
        "id": "j9RYM3Zw3iaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# UNZIP YOUR DATA FROM GOOGLE DRIVE TO GOOGLE COLAB AS IT IS FASTER TO READ IMAGES FROM GOOGLE COLAB DIRECTLY\n",
        "# This step will take maximum 2 mins\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('drive/Datasets/CelebA/Img/img_align_celeba.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SkwrhVqSxGk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "ImgSz = 224\n",
        "\n",
        "## Create function that takes\n",
        "# f: list contains your dataset in this format [image_path /space/ label]\n",
        "# b_start, b_end: starting and ending points of the batch that you want to be extracted\n",
        "# OUTPUT:\n",
        "# images and labels of size (b_end - b_start + 1)\n",
        "\n",
        "def LoadData(f, b_start, b_end):\n",
        "  \n",
        "  random.shuffle(f)\n",
        "  b_end = min(len(f), b_end)\n",
        "  \n",
        "  labels = []\n",
        "  images = []\n",
        "  for i in range(b_start, b_end):\n",
        "    dt = f[i].split(\" \")\n",
        "    #Reading Image\n",
        "    im = image.load_img(dt[0])  \n",
        "    im = im.resize((ImgSz, ImgSz))\n",
        "    x = image.img_to_array(im) \n",
        "    #x = x.reshape((1,) + x.shape) \n",
        "    x = np.array(x, dtype=\"float\") / 255.0\n",
        "\n",
        "    images.append(x)\n",
        "    labels.append([int(lx) for lx in dt[1:]])\n",
        "\n",
        "  lbls = np.array(labels)\n",
        "  imgs = np.array(images)\n",
        "\n",
        "  return imgs, lbls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbUZsHM4xsEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to be given to fit_generator. This function is responsible of \n",
        "# providing fit_generator with batch of images and their labels\n",
        "# INPUT:\n",
        "# f: list contains your dataset in this format [image_path /space/ label]\n",
        "# batch_size: number of images per batch (CNN iteration)\n",
        "def imageLoader(f, batch_size):\n",
        "\n",
        "    numOfImgs = len(f)\n",
        "\n",
        "    # make the generator infinite  \n",
        "    while True:\n",
        "      \n",
        "      batch_start = 0\n",
        "      batch_end = batch_size\n",
        "      #random.shuffle(f)\n",
        "      while batch_start < numOfImgs:\n",
        "          [X, Y] = LoadData(f, batch_start, batch_end)\n",
        "          yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n",
        "\n",
        "          batch_start += batch_size   \n",
        "          batch_end += batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VliwYfswb4c2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Our Model\n",
        "\n",
        "## In this project, we developed a multilabel classifier using pre-trained CNNs.\n",
        "### As a pre-trained model we used VGG16 which is trained using imagenet data.\n",
        "#### Reference: https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
        "\n",
        "## Model Structure:\n",
        "### In this model, we trained the last hidden layer of VGG16 from scratch.\n",
        "### Therefore, we removed one hidden layer + output layer.\n",
        "\n",
        "### The layers we added:\n",
        "#### --Dense layer with 4096 units -> The last hidden layer we trained from scratch.\n",
        "#### --Dropout layer with rate 0.5 -> To prevent overfitting.\n",
        "#### --Dense layer with 10 units and sigmoid activation function. -> 10 attributes, multilabel classification\n",
        "\n",
        "#### Our problem is a multilabel classification problem because we assign multiple targets (attributes) to each image!\n",
        "#### Therefore, sigmoid activation function is used instead of softmax activation function."
      ]
    },
    {
      "metadata": {
        "id": "fkX2qmHwxx80",
        "colab_type": "code",
        "outputId": "2765d653-dd57-42d0-d6ae-6c6c7e5d0a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1632
        }
      },
      "cell_type": "code",
      "source": [
        "# All layers will be freezed except last 2 hidden layers will be trained from scratch\n",
        "\n",
        "#Loading VGG model\n",
        "VGGmodel = VGG16(weights = \"imagenet\")\n",
        "VGGmodel.summary() # Print the network\n",
        "\n",
        "# Freeze the weight of the rest of the layers\n",
        "for layer in VGGmodel.layers[:-4]: # We want to train the last 2 layers of VGG.\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Remove last two layers (one hidden layers + Output layer)\n",
        "VGGmodel.layers.pop()\n",
        "VGGmodel.layers.pop()\n",
        "\n",
        "# Check the status of the layers \n",
        "for layer in VGGmodel.layers:\n",
        "    print(layer, layer.trainable)\n",
        "    \n",
        "# Create the model\n",
        "model = Sequential()\n",
        " \n",
        "# Add the vgg convolutional base model\n",
        "model.add(VGGmodel)\n",
        " \n",
        "# Add new layers\n",
        "model.add(Dense(4096, activation='relu')) # Put a dense layer instead of the hidden layer\n",
        "model.add(Dropout(0.5)) # Put a dropout layer to prevent the overfitting\n",
        "model.add(Dense(10, activation='sigmoid')) # Output layer\n",
        "\n",
        "\n",
        " \n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model.summary()\n",
        "# Check the status of the layers \n",
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 36s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<keras.engine.input_layer.InputLayer object at 0x7f5deb0bddd8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5df2d0d390> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5df2d0d438> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5deb0c3780> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5deb0c35f8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea80d080> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5dea829358> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea8295f8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea82ec88> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea7e02b0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5dea7934e0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea793358> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea7a4c88> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea7482b0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5dea77c4e0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea77c358> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea716c88> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5dea7332b0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5dea6e6518> False\n",
            "<keras.layers.core.Flatten object at 0x7f5dea6e6390> True\n",
            "<keras.layers.core.Dense object at 0x7f5dea703358> True\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 1000)              117479232 \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              4100096   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 121,620,298\n",
            "Trainable params: 106,905,610\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "<keras.engine.training.Model object at 0x7f5dea6506d8> True\n",
            "<keras.layers.core.Dense object at 0x7f5de4d46dd8> True\n",
            "<keras.layers.core.Dropout object at 0x7f5de4d46940> True\n",
            "<keras.layers.core.Dense object at 0x7f5de44d2ba8> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gg9W3zeNyluG",
        "colab_type": "code",
        "outputId": "65ca9740-63b0-40f5-ef17-32fc0fb2f184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls img_align_celeba | grep \"000001.jpg\" # To test whether the images are extracted successfully."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "000001.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0G65lQcfgWH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The code to create train, validation and test sets.\n",
        "\n",
        "### 1. Partition file is read to get which image belongs to which set (train, validation or test).\n",
        "### 2. Attribute file is read to get the labels of each image\n",
        "### 3. Attibutes which have the values \"-1\" are converted to 0 because we use binary_crossentropy.\n",
        "### 4. The attributes we are interested in are taken together with the corresponding image.\n",
        "### 5. Training set is being taken by looking at the 0 values in the partition file.\n",
        "### 6. Training set is written into the text file: \"drive/Datasets/TRAIN_data.txt\"\n",
        "### 7. Validation set is being taken by looking at the values \"1\" in the partition file.\n",
        "### 8. Validation set is written into the text file: \"drive/Datasets/VALIDATION_data.txt\"\n",
        "### 9. Test set is being taken by looking at the values \"2\" in the partition file.\n",
        "### 10. Test set is written into the text file: \"drive/Datasets/TEST_data.txt\""
      ]
    },
    {
      "metadata": {
        "id": "nHUvMo8K5-bt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "while True: # In the first run, Colab may not be able to read the data.\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    eval_file = open('drive/Datasets/CelebA/Eval/list_eval_partition.txt', 'r').read().splitlines() # Read the partition file\n",
        "    eval_file = list(filter(None, eval_file)) # remove any empty cell in the list\n",
        "\n",
        "    labels_file = open('drive/Datasets/CelebA/Anno/list_attr_celeba.txt', 'r').read().splitlines() # Read the attribute file\n",
        "    labels_file = list(filter(None, labels_file)) # remove any empty cell in the list\n",
        "    labels_file = labels_file[2:]\n",
        "\n",
        "    #Attribute Labels: 20, 31, 22, 24, 5, 15, 35, 9, 26, 2, 10\n",
        "    labels_file = [line.split() for line in labels_file]\n",
        "\n",
        "\n",
        "    for index, line in enumerate(labels_file): # -1s are converted to 0s in the attributes\n",
        "      for index2, label in enumerate(line):\n",
        "        if label == \"-1\":\n",
        "          labels_file[index][index2] = \"0\"\n",
        "    \n",
        "\n",
        "    # Interested attributes are taken\n",
        "    labels_file = [line[0] + \" \" + line[21] + \" \" + line[32] + \" \" + str(int(line[23]) or int(not(int(line[25])))) + \" \" + line[6] + \" \" + line[16]\n",
        "                   + \" \" + line[36] + \" \" + line[10] + \" \" + line[27] + \" \" + line[3] + \" \" + line[11] for line in labels_file]\n",
        "\n",
        "    \n",
        "\n",
        "    # Train set is taken\n",
        "    eval_file_train = [eline for eline in eval_file if eline[-1] == '0']\n",
        "    eval_file_train = [eline.split()[0] for eline in eval_file_train]\n",
        "\n",
        "    \n",
        "    # Training data is written into the text file.\n",
        "    with open('drive/Datasets/TRAIN_data.txt', 'w') as f:\n",
        "        for item in labels_file[:int(eval_file_train[-1].split(\".\")[0])]:\n",
        "            f.write(\"%s\\n\" % (\"img_align_celeba/\" + item))\n",
        "\n",
        "    # Validation set is taken\n",
        "    eval_file_validation = [eline for eline in eval_file if eline[-1] == '1']\n",
        "    eval_file_validation = [eline.split()[0] for eline in eval_file_validation]\n",
        "\n",
        "    # Validation data is written into the text file.\n",
        "    with open('drive/Datasets/VALIDATION_data.txt', 'w') as f:\n",
        "        for item in labels_file[int(eval_file_validation[0].split(\".\")[0])-1:int(eval_file_validation[-1].split(\".\")[0])]:\n",
        "            f.write(\"%s\\n\" % (\"img_align_celeba/\" + item))\n",
        "\n",
        "    # Test set is taken\n",
        "    eval_file_test = [eline for eline in eval_file if eline[-1] == '2']\n",
        "    eval_file_test = [eline.split()[0] for eline in eval_file_test]\n",
        "\n",
        "    # Test set is written into the text file\n",
        "    with open('drive/Datasets/TEST_data.txt', 'w') as f:\n",
        "        for item in labels_file[int(eval_file_test[0].split(\".\")[0])-1:]:\n",
        "            f.write(\"%s\\n\" % (\"img_align_celeba/\" + item))\n",
        "\n",
        "            \n",
        "    # Read text file contains training data in a list\n",
        "    f_train = open('drive/Datasets/TRAIN_data.txt', 'r').read().splitlines()\n",
        "    f_train = list(filter(None, f_train)) # remove any empty cell in the list\n",
        "\n",
        "    # Read text file contains testing data in a list\n",
        "    f_validation = open('drive/Datasets/VALIDATION_data.txt', 'r').read().splitlines()\n",
        "    f_validation = list(filter(None, f_validation)) # remove any empty cell in the list\n",
        "\n",
        "    # Read text file contains testing data in a list\n",
        "    f_test = open('drive/Datasets/TEST_data.txt', 'r').read().splitlines()\n",
        "    f_test = list(filter(None, f_test)) # remove any empty cell in the list\n",
        "    \n",
        "    if(f_train[0][:3] == \"img\" and f_validation[0][:3] == \"img\" and f_test[0][:3] == \"img\"):\n",
        "      break\n",
        "    \n",
        "  except:\n",
        "    \n",
        "    continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIUVDJmDHynZ",
        "colab_type": "code",
        "outputId": "f3b993b6-1ced-4c19-c5ff-c36e0a3956b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "f_train[:5] # To see that the train set is read successfully."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['img_align_celeba/000001.jpg 0 1 0 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/000002.jpg 0 1 0 0 0 0 0 0 0 0',\n",
              " 'img_align_celeba/000003.jpg 1 0 0 0 0 0 0 0 0 1',\n",
              " 'img_align_celeba/000004.jpg 0 0 0 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/000005.jpg 0 0 0 0 0 0 0 0 1 0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "mmbYbFOdNAn2",
        "colab_type": "code",
        "outputId": "056d9c2f-c1e7-46c0-8c21-86083888345d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "f_validation[:5] # To see that the validation set is read successfully."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['img_align_celeba/162771.jpg 0 1 0 1 0 0 0 0 1 0',\n",
              " 'img_align_celeba/162772.jpg 1 1 1 0 0 0 0 0 0 0',\n",
              " 'img_align_celeba/162773.jpg 0 0 0 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/162774.jpg 1 1 1 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/162775.jpg 0 0 0 0 0 0 0 0 0 0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "I6H_CP8dhFBe",
        "colab_type": "code",
        "outputId": "a5901e31-b587-4d73-d2ca-d64d7b5d80da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "f_test[:5] # # To see that the test set is read successfully."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['img_align_celeba/182638.jpg 0 1 0 0 0 1 0 0 0 0',\n",
              " 'img_align_celeba/182639.jpg 0 0 0 0 0 0 0 0 0 0',\n",
              " 'img_align_celeba/182640.jpg 0 1 0 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/182641.jpg 0 1 0 0 0 0 0 0 1 0',\n",
              " 'img_align_celeba/182642.jpg 0 1 0 0 0 0 0 0 0 0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "W6xKSGonivqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Compilation\n",
        "\n",
        "### Epoch = 9\n",
        "#### -- Even though we used earlystopping technique, we wanted to train at most 9 epochs because Google Colab's GPU was -- running out of memory and it did not allow us to train the model more previously.\n",
        "\n",
        "### Optimizer = SGD with learning rate 0.1 because we also tried Adam but SGD gave a better result in general.\n",
        "### Loss function = binary_crossentropy because we developed a multilabel classifier which targets multiple labels at the same time.\n",
        "#### Metrics = Binary accuracy is used to get the general training accuracy for each target.\n",
        "\n",
        "### **In our model, earlystopping technique is utilized based on the validation loss to prevent the overfitting and have better generalized model.\n",
        "\n",
        "### **Therefore, we did not need to check the validation data after the model is trained. We take the model which generalizes the best on the validation data.\n",
        "\n",
        "### **We do not train our model on the test data! Validation set is used to see the generalization!\n",
        "\n",
        "### Notice: Since our model generalized better at each epoch more, it could reached until epoch 8 without stopping. Maybe it would be better to train more but Google Colab did not allow us to do that because of memory constraints and stopped training at the 8th epoch."
      ]
    },
    {
      "metadata": {
        "id": "3XJq2KSIokgL",
        "colab_type": "code",
        "outputId": "226e6d38-c229-43f1-e667-50e4b2e24d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 9\n",
        "batch_size = 20\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(0.1),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# TRAINING\n",
        "model.fit_generator(imageLoader(f_train, batch_size), steps_per_epoch=len(f_train)/batch_size, nb_epoch = epochs, \n",
        "                    verbose=1, validation_data=imageLoader(f_validation, batch_size), validation_steps=len(f_validation)/batch_size, callbacks=[earlystopper])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=8138.5, verbose=1, validation_data=<generator..., validation_steps=993.35, callbacks=[<keras.ca..., epochs=9)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "8139/8138 [==============================] - 2883s 354ms/step - loss: 0.3726 - acc: 0.8275 - val_loss: 0.3164 - val_acc: 0.8577\n",
            "Epoch 2/9\n",
            "8139/8138 [==============================] - 2893s 355ms/step - loss: 0.3058 - acc: 0.8657 - val_loss: 0.2986 - val_acc: 0.8686\n",
            "Epoch 3/9\n",
            "8139/8138 [==============================] - 2913s 358ms/step - loss: 0.2810 - acc: 0.8833 - val_loss: 0.2706 - val_acc: 0.8849\n",
            "Epoch 4/9\n",
            "8139/8138 [==============================] - 2916s 358ms/step - loss: 0.2592 - acc: 0.8928 - val_loss: 0.2488 - val_acc: 0.8936\n",
            "Epoch 5/9\n",
            "8139/8138 [==============================] - 2924s 359ms/step - loss: 0.2476 - acc: 0.8967 - val_loss: 0.2431 - val_acc: 0.8984\n",
            "Epoch 6/9\n",
            "8139/8138 [==============================] - 2913s 358ms/step - loss: 0.2390 - acc: 0.9013 - val_loss: 0.2359 - val_acc: 0.9032\n",
            "Epoch 7/9\n",
            "8139/8138 [==============================] - 2915s 358ms/step - loss: 0.2324 - acc: 0.9051 - val_loss: 0.2254 - val_acc: 0.9082\n",
            "Epoch 8/9\n",
            "1823/8138 [=====>........................] - ETA: 34:23 - loss: 0.2282 - acc: 0.9079Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bkwx47FTl9l0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing on the test data to see the binary accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "lxa1B0WjhjLf",
        "colab_type": "code",
        "outputId": "b65cff2c-2e41-4d51-d8fb-bb50dec73a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate_generator(imageLoader(f_test, batch_size), steps = len(f_test)/batch_size)\n",
        "print('Final test accuracy:', (scores[1]*100.0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test accuracy: 89.805128430809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fTZnKadwmGgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The functions below are written to get the accuracy for each individual attribute.\n",
        "\n",
        "### 1. getTargetPred function takes a line (data for an img) from the test set and returns the predictions of the attributes together with its true attributes."
      ]
    },
    {
      "metadata": {
        "id": "Z8xyanOyxzNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getTargetPred(line):\n",
        "  \n",
        "  dt = line.split(\" \")\n",
        "    \n",
        "  im = image.load_img(dt[0])  \n",
        "  im = im.resize((ImgSz, ImgSz))\n",
        "  x = image.img_to_array(im) \n",
        "  #x = x.reshape((1,) + x.shape) \n",
        "  x = np.array(x, dtype=\"float\") / 255.0\n",
        "    \n",
        "  img = np.array([x])\n",
        "  y_pred = model.predict(img)\n",
        "  y_target = np.array([[int(lx) for lx in dt[1:]]])\n",
        "  return y_target[0], y_pred[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CM6LYnmpmtMS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. getPerAcc function takes the test set file and calculates the accuracies for each individual attribute together with the average accuracy of them."
      ]
    },
    {
      "metadata": {
        "id": "0Yp6QTUh2u-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getPerAcc(f):\n",
        "  print(\"Calculating the accuracy of each attribute.\")\n",
        "  print(\"This may take about 10 minutes...\")\n",
        "  accs = [0] * 10\n",
        "  for ind, line in enumerate(f):\n",
        "    y_target, y_pred = getTargetPred(line)\n",
        "    \n",
        "    for index in range(len(y_pred)):\n",
        "      if (y_pred[index] <= 0.5 and y_target[index] == 0) or (y_pred[index] > 0.5 and y_target[index] == 1):\n",
        "        accs[index] += 1\n",
        "  \n",
        "  print()\n",
        "  print(\"Accuracy of each attribute:\")\n",
        "  numS = len(f)\n",
        "  #Attribute Labels: 20, 31, 22-24, 5, 15, 35, 9, 26, 2, 10\n",
        "  attLabels = {0:\"male/female\", 1:\"smile/not\" , 2:\"mustache-beard/not\", 3:\"bangs/not\" , \n",
        "               4:\"eyeglasses/not\" , 5:\"hat/not\" , 6:\"blonde/not\" , 7:\"pale skin\", 8:\"attractive/not\", 9:\"blurry/not\"}\n",
        "  \n",
        "  totalAcc = 0\n",
        "  for inn, cl in enumerate(accs):\n",
        "    totalAcc += cl/numS\n",
        "    print(attLabels[inn] + \": \" + str(cl/numS))\n",
        "  \n",
        "  print()\n",
        "  print(\"Average Accuracy:\", totalAcc/float(len(accs)))\n",
        "   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yi0zVwIXnBb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Result:\n",
        "\n",
        "### 1. Accuracy per attribute\n",
        "### 2. Average accuracy"
      ]
    },
    {
      "metadata": {
        "id": "IkOVyw2k5253",
        "colab_type": "code",
        "outputId": "27cdff30-6183-4fe7-e94d-ed9c75d55e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "getPerAcc(f_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating the accuracy of each attribute.\n",
            "This may take about 10 minutes...\n",
            "\n",
            "Accuracy of each attribute:\n",
            "male/female: 0.8979060214407374\n",
            "smile/not: 0.8293758140466887\n",
            "mustache-beard/not: 0.8810740406772869\n",
            "bangs/not: 0.9268109407874963\n",
            "eyeglasses/not: 0.9466486324015629\n",
            "hat/not: 0.9577697625488428\n",
            "blonde/not: 0.8926460274521592\n",
            "pale skin: 0.9579200480913737\n",
            "attractive/not: 0.7353972547840898\n",
            "blurry/not: 0.9494038673479611\n",
            "\n",
            "Average Accuracy: 0.8974952409578197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z8y5AjaYnPTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "### Our model performed pretty well for the attributes except the attribute \"attractiveness\".\n",
        "### Average accuracy is also so close to 90% and it is pretty well in our own opinion.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HIGKNLRezOIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}